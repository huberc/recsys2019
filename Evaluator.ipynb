{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import SessionItemMatrix as sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Recommender_CB import ContentBasedRecommender\n",
    "from Recommender_CF_UU import UUCFRecommender\n",
    "from Recommender_MF import MFRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christoph/Dokumente/recsys2019/SessionItemMatrix.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  one_hot_action_type['clickout item'] * 2.0\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(data_directory + 'train_subsample.csv')\n",
    "train = sim.prepare_dataset(train)\n",
    "test = pd.read_csv(data_directory + 'test_subsample.csv')\n",
    "test = sim.prepare_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "if DEBUG:\n",
    "    random.seed(42)\n",
    "\n",
    "class Evaluator:\n",
    "    \n",
    "    def __init__(self, topN=20):\n",
    "        self.topN = topN\n",
    "\n",
    "    \n",
    "    def init_data(self, train, test):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.find_unweighted_items()\n",
    "    \n",
    "    \n",
    "    ### store for each session her/his unrated items\n",
    "    def find_unweighted_items(self):\n",
    "        all_items = set(self.train['reference'].tolist())\n",
    "    \n",
    "        self.unweighted = {}\n",
    "        \n",
    "        for session_id in self.train['session_id'].unique():\n",
    "            print(session_id)\n",
    "            weighted_train_items = self.train[self.train['session_id'] == session_id]['reference'].tolist()\n",
    "            weighted_test_items = self.test[self.test['session_id'] == session_id]['reference'].tolist()\n",
    "\n",
    "            weighted_items = set(weighted_train_items) | set(weighted_test_items) # union of sets\n",
    "            unweighted_items = list(all_items - weighted_items)\n",
    "            random.shuffle(unweighted_items)\n",
    "            \n",
    "            print(unweighted_items)\n",
    "            self.unweighted[session_id] = unweighted_items\n",
    "            \n",
    "    \n",
    "    ### get the weights of session_id in ratings_test\n",
    "    def get_ground_truth(self, session_id):\n",
    "        \n",
    "        ## get the test ratings of session_id as a DataFrame subset of `self.ratings_test`\n",
    "        # YOUR CODE HERE\n",
    "        session_weights = self.test.loc[self.test['session_id'] == session_id]\n",
    "        \n",
    "        ## dictionary of ground truth ratings\n",
    "        ground_truth = pd.Series(session_weights['weights'].values, index=session_weights['reference']).to_dict()\n",
    "\n",
    "        return ground_truth\n",
    "    \n",
    "    \n",
    "    def get_recommendations(self, model, session_id):\n",
    "        ground_truth = self.get_ground_truth(session_id)\n",
    "        n_test = len(ground_truth)\n",
    "        \n",
    "        ## we will create a total of topN items, and ask the recommender to rank them\n",
    "        ## among these items, we will include the ground truth items\n",
    "        \n",
    "        ## 1. select (topN - n_test) unrated items\n",
    "        item_ids = self.unweighted[session_id][:self.topN - n_test]\n",
    "        \n",
    "        ## 2; add ground truth items\n",
    "        item_ids = item_ids + list(ground_truth.keys())\n",
    "        \n",
    "        ## get the model's weights\n",
    "        recommendations = model.recommend(session_id, item_ids, self.topN)\n",
    "        return recommendations\n",
    "    \n",
    "    \n",
    "    ### evaluate the model on given session_id\n",
    "    def eval_model_on_session(self, model, session_id):\n",
    "        ground_truth = self.get_ground_truth(session_id)\n",
    "        n_test = len(ground_truth)\n",
    "        \n",
    "        ## we will create a total of topN items, and ask the recommender to rank them\n",
    "        ## among these items, we will include the ground truth items\n",
    "        \n",
    "        ## 1. select (topN - n_test) unrated items\n",
    "        item_ids = self.unweighted[session_id][:self.topN - n_test]\n",
    "        \n",
    "        ## 2; add ground truth items\n",
    "        item_ids = item_ids + list(ground_truth.keys())\n",
    "        \n",
    "        ## get the model's weights\n",
    "        recommendations = model.recommend(session_id, item_ids, self.topN)\n",
    "        \n",
    "        ## evaluate the weights\n",
    "        metrics = self.get_weights_metrics(ground_truth, recommendations)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    ### evaluate the model on all sessions\n",
    "    def eval_model(self, model, n_sessions=-1):\n",
    "        metrics_all = []\n",
    "        count = 0;\n",
    "        for session_id in self.train['session_id'].unique():\n",
    "            count+=1\n",
    "            print(\"\\r\", \"evaluated on \", count, \" sessions\", end=\"\", sep=\"\")\n",
    "            metrics = self.eval_model_on_session(model, session_id)\n",
    "            if metrics is None:\n",
    "                continue\n",
    "            metrics_all.append(metrics)\n",
    "            if count == n_sessions:\n",
    "                break\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        \n",
    "        \n",
    "        ## store all metrics in a DataFrame for easy manipulation\n",
    "        metrics_all_df = pd.DataFrame(metrics_all)\n",
    "        self.metrics_all_df = metrics_all_df        \n",
    "        \n",
    "        ## average over all metrics\n",
    "        hits_array = metrics_all_df.hits\n",
    "        hits = np.nanmean(hits_array)\n",
    "        ap_array = metrics_all_df.ap\n",
    "        ap = np.nanmean(ap_array)\n",
    "        \n",
    "        rec_array = np.vstack(metrics_all_df.rec)\n",
    "        prec_array = np.vstack(metrics_all_df.prec)\n",
    "        ndcg_array = np.vstack(metrics_all_df.ndcg)\n",
    "        \n",
    "        \n",
    "        with warnings.catch_warnings(): ## ignore division by 0\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            rec = np.nanmean(rec_array, axis=0)\n",
    "            prec = np.nanmean(prec_array, axis=0)\n",
    "            ndcg = np.nanmean(ndcg_array, axis=0)\n",
    "        \n",
    "        \n",
    "        metrics_avg = {'hits':hits,\n",
    "                   'ap':ap,\n",
    "                   'rec':np.array(rec),\n",
    "                   'prec':np.array(prec),\n",
    "                   'ndcg':np.array(ndcg)}\n",
    "        \n",
    "        return metrics_avg\n",
    "        \n",
    "    ### get some evaluation metrics for weights with respect to ground_truth\n",
    "    def get_weights_metrics(self, ground_truth, weights):\n",
    "        n_test = len(ground_truth)\n",
    "        if n_test == 0:\n",
    "            return None\n",
    "        \n",
    "        hits = 0 ## number of relevant in weights\n",
    "        rec = [] ## recall at every position of weights\n",
    "        prec = [] ## precision at every position of weights\n",
    "        dcg = [] ## DCG at every position of weights\n",
    "        ap = 0 ## average precision\n",
    "        \n",
    "        \n",
    "        ## scan the weights and compute hits, rec, prec, dcg, ap\n",
    "        runs = 1\n",
    "        for x in weights:\n",
    "            if(x in ground_truth):\n",
    "                hits+=1\n",
    "                ap += (hits/runs)\n",
    "                relevance = ground_truth.get(x)\n",
    "            else:\n",
    "                relevance = 0.\n",
    "            prec.append(hits/runs)\n",
    "            runs+=1\n",
    "            numerator = math.pow(2,relevance) - 1\n",
    "            denumerator = math.log(runs,2)\n",
    "            if(len(dcg) > 0):\n",
    "                dcg.append(np.array(dcg)[len(dcg)-1]+numerator/denumerator)\n",
    "            else:\n",
    "                dcg.append(numerator/denumerator)\n",
    "                \n",
    "        hitsForRecall = 0\n",
    "        for x in weights:\n",
    "            if(x in ground_truth):\n",
    "                hitsForRecall += 1\n",
    "            rec.append(hitsForRecall/hits)\n",
    "                \n",
    "        if (hits != 0):\n",
    "            ap /= hits\n",
    "        else:\n",
    "            ap = 0\n",
    "            \n",
    "        ## constuct the ideal weights from ground truth to compute idcg\n",
    "        ideal = sorted(ground_truth, key=ground_truth.get, reverse=True)\n",
    "        idcg = []\n",
    "        \n",
    "        ## scan the ideal weights and compute idcg\n",
    "        runs = 1\n",
    "        for x in ideal:\n",
    "            relevance = ground_truth.get(x)\n",
    "            numerator = math.pow(2,relevance) - 1\n",
    "            denumerator = math.log(runs+1,2)\n",
    "            runs+=1\n",
    "            if(len(idcg) > 0):\n",
    "                idcg.append(np.array(idcg)[len(idcg)-1]+numerator/denumerator)\n",
    "            else:\n",
    "                idcg.append(numerator/denumerator)\n",
    "                \n",
    "        for x in range(0,(len(dcg)-len(idcg))):\n",
    "            idcg.append(np.array(idcg)[len(idcg)-1])\n",
    "            \n",
    "        ## make sure the dcg and idcg lists have the same length\n",
    "        if len(ideal) >= len(weights):\n",
    "            idcg = idcg[:len(weights)]\n",
    "        else:\n",
    "            last_idcg = idcg[-1]\n",
    "            for i in range(len(weights) - len(ideal)):\n",
    "                idcg.append(last_idcg)\n",
    "        \n",
    "        ## compute NDCG = DCG/IDCG\n",
    "        ## TIP convert lists to `np.array` to do the division and then back to a list with `.tolist()`\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        ndcg = []\n",
    "        for x in range(0,len(dcg)):\n",
    "            ndcg.append(dcg[x]/idcg[x])\n",
    "            \n",
    "        rec = np.array(rec)\n",
    "        prec = np.array(prec)\n",
    "        ndcg = np.array(ndcg)\n",
    "        \n",
    "        ## make them have length self.topN, fill in with nan \n",
    "        rec = np.append(rec, np.repeat(np.nan, self.topN - len(rec)))\n",
    "        prec = np.append(prec, np.repeat(np.nan, self.topN - len(prec)))\n",
    "        ndcg = np.append(ndcg, np.repeat(np.nan, self.topN - len(ndcg)))\n",
    "        \n",
    "        metrics = {'hits':hits,\n",
    "                   'ap':ap,\n",
    "                   'rec':np.array(rec),\n",
    "                   'prec':np.array(prec),\n",
    "                   'ndcg':np.array(ndcg)}\n",
    "        \n",
    "        return metrics"
   ]
  },
   ],
   "source": [
    "evl = Evaluator(topN = 20)\n",
    "evl.init_data(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
