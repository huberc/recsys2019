{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item Embeddings\n",
    "**Please note:** This Notebook uses scala to execute spark code. All operations and calculations were done with the scala spylon-kernel\n",
    "\n",
    "The more recent Spark implementation of Word2Vec in package org.apache.spark.ml.feature is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-2NBBC4T:4041\n",
       "SparkContext available as 'sc' (version = 2.4.3, master = local[*], app id = local-1563036190343)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\r\n",
       "import org.apache.spark.ml.feature.{Word2Vec, Word2VecModel}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.ml.feature.{Word2Vec, Word2VecModel}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All parameters are set here:\n",
    "\n",
    "    inputCol: Is needed by the model as parameter. Must not be changed!\n",
    "    maxIter: Number of epochs for the model\n",
    "    minCount: Number of minimum occurrences of a word to be considered in the model. (i.e: minCount=5 and some word only occures twice -> word is not considered in the model)\n",
    "    numPartitions: Number of data partitions\n",
    "    seed: Seed for reproducability\n",
    "    vectorSize: dimensionality of output vector\n",
    "    debug: If yes subset of samples are taken\n",
    "    ratio: if debug is set this sets the ratio of sampling 0 < x < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputCol: String = text\r\n",
       "maxIter: Int = 1\r\n",
       "minCount: Int = 10\r\n",
       "numPartitions: Int = 1\r\n",
       "seed: Int = 42\r\n",
       "vectorSize: Int = 15\r\n",
       "debug: Boolean = false\r\n",
       "ratio: Double = 0.1\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputCol = \"text\"\n",
    "val maxIter = 1\n",
    "val minCount = 10\n",
    "val numPartitions = 1\n",
    "val seed = 42\n",
    "val vectorSize = 15\n",
    "\n",
    "val debug = false\n",
    "val ratio = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for printing RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "printRDD: [T](rdd: org.apache.spark.rdd.RDD[T], n: Int)Unit\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def printRDD[T] ( rdd:RDD[T], n:Int = 0 ) : Unit = {\n",
    "    if(n != 0) {\n",
    "    rdd.take(n).foreach(println)\n",
    "} else {\n",
    "    rdd.collect().foreach(println)}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function getting the Word2VecModel instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getModel: ()org.apache.spark.ml.feature.Word2Vec\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getModel() : Word2Vec = {\n",
    "    var word2vec = new Word2Vec()\n",
    "    word2vec.setInputCol(inputCol)\n",
    "    word2vec.setMaxIter(maxIter)\n",
    "    word2vec.setMinCount(minCount)\n",
    "    word2vec.setNumPartitions(numPartitions)\n",
    "    word2vec.setSeed(seed)\n",
    "    word2vec.setVectorSize(vectorSize)\n",
    "    return word2vec;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the dataset from ../data/item_metadata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.rdd.RDD[String] = ../data/item_metadata.csv MapPartitionsRDD[1] at textFile at <console>:27\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var data = sc.textFile(\"../data/item_metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets comes with a first line header. This needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "header: String = item_id,properties\r\n",
       "data: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at <console>:33\r\n",
       "res0: Long = 927142\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val header = data.first()\n",
    "data = data.filter(row => row != header)\n",
    "if(debug) {\n",
    "data = data.sample(false, ratio, seed)\n",
    "}\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column of the metadata dataset does not belong into the model. It is an external id and is removed.\n",
    "\n",
    "Then the data is split by each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "truncated_data: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at map at <console>:28\r\n",
       "preprocessed_Data: org.apache.spark.sql.DataFrame = [text: array<string>]\r\n",
       "words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[9] at distinct at <console>:30\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val truncated_data = data.map(_.split(',')(1))\n",
    "val preprocessed_Data = truncated_data.map(_.split('|')).map(Tuple1.apply).toDF(\"text\")\n",
    "val words = truncated_data.flatMap(_.split('|')).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct beach access\n",
      "Guest House\n",
      "Ironing Board\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res1: Long = 927142\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printRDD(words, 3)\n",
    "words.count()\n",
    "preprocessed_Data.head(3)\n",
    "preprocessed_Data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a new Word2Vec instalnce and fit the model to the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.feature.Word2VecModel = w2v_17eeb45b2426\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = getModel().fit(preprocessed_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the 10 most similar words to the term Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[org.apache.spark.sql.Row] = Array([Swimming Pool (Outdoor),0.9618108868598938], [Spa Hotel,0.94759202003479], [Ski Resort,0.945956826210022], [Health Retreat,0.9367016553878784], [Nightclub,0.9317147731781006], [Cot,0.930616557598114], [Casino (Hotel),0.922082245349884], [Szep Kartya,0.8853012919425964], [Beach,0.8825615644454956], [Hypoallergenic Bedding,0.8758768439292908])\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.findSynonyms(\"Gym\", 10).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally save the model and the vectors on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write.overwrite.save(\"word2vec-model\")\n",
    "model.getVectors.rdd.saveAsTextFile(\"vectors-out\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
